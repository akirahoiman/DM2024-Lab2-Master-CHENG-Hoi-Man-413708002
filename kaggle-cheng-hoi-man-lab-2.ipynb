{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#1.數據預處理\n#1-1數據清洗-缺失值處理\n\nimport pandas as pd\nimport json\n\n# 定義檔路徑\ndata_identification_path = r'/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv'\nemotion_path = r'/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv'\n\n# 讀取數據\ndata_identification = pd.read_csv(data_identification_path)\nemotion = pd.read_csv(emotion_path)\n\n# 1-1 資料清洗 - 缺失值處理\n# 檢查data_identification.csv中的缺失值\nmissing_data_identification = data_identification.isnull().sum()\nprint(\"缺失值統計 - data_identification.csv:\")\nprint(missing_data_identification)\n\n# 檢查emotion.csv中的缺失值\nmissing_emotion = emotion.isnull().sum()\nprint(\"缺失值統計 - emotion.csv:\")\nprint(missing_emotion)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-03T15:38:07.266088Z","iopub.execute_input":"2024-12-03T15:38:07.267715Z","iopub.status.idle":"2024-12-03T15:38:10.628763Z","shell.execute_reply.started":"2024-12-03T15:38:07.267608Z","shell.execute_reply":"2024-12-03T15:38:10.627587Z"},"trusted":true},"outputs":[{"name":"stdout","text":"缺失值統計 - data_identification.csv:\ntweet_id          0\nidentification    0\ndtype: int64\n缺失值統計 - emotion.csv:\ntweet_id    0\nemotion     0\ndtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 1-2 資料劃分 - 訓練集和測試集\n# 根據identification列將資料分為訓練集和測試集\ntrain_data = data_identification[data_identification['identification'] == 'train'].copy()\ntest_data = data_identification[data_identification['identification'] == 'test'].copy()\n\n# 檢查訓練集和測試集中的tweet_id是否有重疊\n重疊tweet_id = train_data['tweet_id'].isin(test_data['tweet_id']).sum()\nprint(\"訓練集和測試集中重疊的tweet_id數量:\", 重疊tweet_id)\n\n# 如果有重疊，列印出重疊的tweet_id\nif 重疊tweet_id > 0:\n    overlapping_ids = train_data[train_data['tweet_id'].isin(test_data['tweet_id'])]['tweet_id'].unique()\n    print(\"重疊的tweet_id列表:\", overlapping_ids)\nelse:\n    print(\"訓練集和測試集中的tweet_id沒有重疊。\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:38:14.601253Z","iopub.execute_input":"2024-12-03T15:38:14.601744Z","iopub.status.idle":"2024-12-03T15:38:15.629785Z","shell.execute_reply.started":"2024-12-03T15:38:14.601701Z","shell.execute_reply":"2024-12-03T15:38:15.628236Z"},"trusted":true},"outputs":[{"name":"stdout","text":"訓練集和測試集中重疊的tweet_id數量: 0\n訓練集和測試集中的tweet_id沒有重疊。\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 1-3 合併資料集\n# 使用tweet_id將train_data和emotion.csv合併，確保每個推文都有對應的情緒標籤\nmerged_train_data = pd.merge(train_data, emotion, on='tweet_id', how='left')\n\n# 檢查合併後的缺失值\nmissing_values = merged_train_data.isnull().sum()\nprint(\"合併後的缺失值統計:\")\nprint(missing_values)\n\n# 查看合併後的資料集前幾行\nprint(merged_train_data.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:38:18.476514Z","iopub.execute_input":"2024-12-03T15:38:18.476976Z","iopub.status.idle":"2024-12-03T15:38:20.667540Z","shell.execute_reply.started":"2024-12-03T15:38:18.476935Z","shell.execute_reply":"2024-12-03T15:38:20.666299Z"},"trusted":true},"outputs":[{"name":"stdout","text":"合併後的缺失值統計:\ntweet_id          0\nidentification    0\nemotion           0\ndtype: int64\n   tweet_id identification       emotion\n0  0x29e452          train           joy\n1  0x2b3819          train           joy\n2  0x2a2acc          train         trust\n3  0x2a8830          train           joy\n4  0x20b21d          train  anticipation\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#1-4文本資料讀取\nimport os\n\n# 定義JSON檔路徑\ntweets_dm_path = r'/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json'\n\n# 初始化一個空字典來存儲推文資料\ntweets_data = {}\n\n# 讀取JSON文件\nwith open(tweets_dm_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        try:\n            # 將每行解析為JSON對象\n            tweet = json.loads(line)\n            # 提取tweet_id和推文文本\n            tweet_id = tweet['_source']['tweet']['tweet_id']\n            text = tweet['_source']['tweet']['text']\n            # 將推文文本存儲在字典中\n            tweets_data[tweet_id] = text\n        except json.JSONDecodeError as e:\n            # 如果解析出錯，列印錯誤資訊和對應的行\n            print(f\"Error parsing line: {line}\")\n            print(e)\n\n# 將推文文本資料轉換為DataFrame\ntweets_df = pd.DataFrame(list(tweets_data.items()), columns=['tweet_id', 'text'])\n\n# 將推文文本資料與合併後的資料集關聯，使用tweet_id作為關聯鍵\nfinal_data = pd.merge(merged_train_data, tweets_df, on='tweet_id', how='left')\n\n# 檢查最終關聯後的資料集前幾行\nprint(final_data.head())\n\n# 將推文文本資料與合併後的資料集關聯，使用tweet_id作為關聯鍵\nfinal_test_data = pd.merge(test_data, tweets_df, on='tweet_id', how='left')\n\n# 檢查最終關聯後的資料集前幾行\nprint(final_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:38:31.878694Z","iopub.execute_input":"2024-12-03T15:38:31.879189Z","iopub.status.idle":"2024-12-03T15:38:51.545582Z","shell.execute_reply.started":"2024-12-03T15:38:31.879148Z","shell.execute_reply":"2024-12-03T15:38:51.544360Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  \\\n0  0x29e452          train           joy   \n1  0x2b3819          train           joy   \n2  0x2a2acc          train         trust   \n3  0x2a8830          train           joy   \n4  0x20b21d          train  anticipation   \n\n                                                text  \n0  Huge Respect🖒 @JohnnyVegasReal talking about l...  \n1  Yoooo we hit all our monthly goals with the ne...  \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  \n3  Come join @ambushman27 on #PUBG while he striv...  \n4  @fanshixieen2014 Blessings!My #strength little...  \n   tweet_id identification                                               text\n0  0x28cc61           test  @Habbo I've seen two separate colours of the e...\n1  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...\n2  0x2466f6           test  Looking for a new car, and it says 1 lady owne...\n3  0x23f9e9           test  @cineworld “only the brave” just out and fount...\n4  0x1fb4e1           test  Felt like total dog 💩 going into open gym and ...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#2.特徵工程\n#2-1文本特徵(詞幹提取)\nimport nltk\nfrom nltk.stem import PorterStemmer\n\n# 初始化詞幹提取器\nstemmer = PorterStemmer()\n\n# 定義詞幹提取函數\ndef stemming(text):\n    words = text.split()\n    stemmed_words = [stemmer.stem(word) for word in words]\n    return ' '.join(stemmed_words)\n\n# 應用文本特徵提取函數到推文文本列\nfinal_data['text_stemmed'] = final_data['text'].apply(stemming)\n\n# 檢查最終關聯後的資料集前幾行\nprint(final_data.head())\n\n# 應用文本特徵提取函數到推文文本列\nfinal_test_data['text_stemmed'] = final_test_data['text'].apply(stemming)\n\n# 檢查最終關聯後的資料集前幾行\nprint(final_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:39:10.201146Z","iopub.execute_input":"2024-12-03T15:39:10.201575Z","iopub.status.idle":"2024-12-03T15:47:13.884625Z","shell.execute_reply.started":"2024-12-03T15:39:10.201521Z","shell.execute_reply":"2024-12-03T15:47:13.883619Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  \\\n0  0x29e452          train           joy   \n1  0x2b3819          train           joy   \n2  0x2a2acc          train         trust   \n3  0x2a8830          train           joy   \n4  0x20b21d          train  anticipation   \n\n                                                text  \\\n0  Huge Respect🖒 @JohnnyVegasReal talking about l...   \n1  Yoooo we hit all our monthly goals with the ne...   \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n3  Come join @ambushman27 on #PUBG while he striv...   \n4  @fanshixieen2014 Blessings!My #strength little...   \n\n                                        text_stemmed  \n0  huge respect🖒 @johnnyvegasr talk about lose hi...  \n1  yoooo we hit all our monthli goal with the new...  \n2  @kidsnt @picu_bch @uhbcomm @bwchboss well done...  \n3  come join @ambushman27 on #pubg while he striv...  \n4  @fanshixieen2014 blessings!mi #strength little...  \n   tweet_id identification                                               text  \\\n0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n1  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n2  0x2466f6           test  Looking for a new car, and it says 1 lady owne...   \n3  0x23f9e9           test  @cineworld “only the brave” just out and fount...   \n4  0x1fb4e1           test  Felt like total dog 💩 going into open gym and ...   \n\n                                        text_stemmed  \n0  @habbo i'v seen two separ colour of the eleg f...  \n1  @foxnew @kellyannepol No seriou self respect i...  \n2  look for a new car, and it say 1 ladi owner. t...  \n3  @cineworld “onli the brave” just out and fount...  \n4  felt like total dog 💩 go into open gym and had...  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#2-2中繼資料特徵(時間特徵、使用者行為特徵、tweet長度)\nimport datetime\n\n# 定義提取時間特徵的函數\ndef extract_time_features(date_str):\n    date = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n    return {\n        'year': date.year,\n        'month': date.month,\n        'day': date.day,\n        'weekday': date.weekday(),  # 0 is Monday, 6 is Sunday\n        'hour': date.hour\n    }\n\n# 定義提取使用者行為特徵的函數\ndef extract_user_behavior_features(text):\n    hashtags = text.count('#')\n    retweet = text.count('RT ')\n    reply = text.count('@')\n    return {\n        'hashtags_count': hashtags,\n        'retweets_count': retweet,\n        'replies_count': reply\n    }\n\n# 定義提取tweet長度特徵的函數\ndef tweet_length(text):\n    return len(text)\n\n# 讀取tweets_DM.json檔並提取特徵\ntweets_features = []\n\nwith open(tweets_dm_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        try:\n            tweet = json.loads(line)\n            tweet_id = tweet['_source']['tweet']['tweet_id']\n            crawl_date = tweet['_crawldate']\n            \n            # 提取時間特徵\n            time_features = extract_time_features(crawl_date)\n            \n            # 將特徵組合在一起\n            tweets_features.append({\n                'tweet_id': tweet_id,\n                **time_features\n            })\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing line: {line}\")\n            print(e)\n\n# 將特徵轉換為DataFrame\ntweets_time_features_df = pd.DataFrame(tweets_features)\n\n# 合併時間特徵到tweets_df\ntweets_df = pd.merge(tweets_df, tweets_time_features_df, on='tweet_id', how='left')\n\n# 提取使用者行為特徵和tweet長度特徵\ntweets_df['user_behavior_features'] = tweets_df['text'].apply(extract_user_behavior_features)\ntweets_df['tweet_length'] = tweets_df['text'].apply(tweet_length)\n\n# 展開使用者行為特徵\nuser_behavior_columns = ['hashtags_count', 'retweets_count', 'replies_count']\ntweets_df[user_behavior_columns] = pd.DataFrame(tweets_df['user_behavior_features'].tolist(), index=tweets_df.index)\n\n# 刪除臨時列\ntweets_df.drop(['user_behavior_features'], axis=1, inplace=True)\n\n# 合併最終資料集\nfinal_data = pd.merge(final_data, tweets_df, on='tweet_id', how='left')\n\n# 檢查最終資料集的前幾行\nprint(final_data.head())\n\n# 合併最終資料集\nfinal_test_data = pd.merge(final_test_data, tweets_df, on='tweet_id', how='left')\n\n# 檢查最終資料集的前幾行\nprint(final_test_data.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:56:12.120696Z","iopub.execute_input":"2024-12-03T15:56:12.121112Z","iopub.status.idle":"2024-12-03T15:57:02.444796Z","shell.execute_reply.started":"2024-12-03T15:56:12.121078Z","shell.execute_reply":"2024-12-03T15:57:02.443674Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  \\\n0  0x29e452          train           joy   \n1  0x2b3819          train           joy   \n2  0x2a2acc          train         trust   \n3  0x2a8830          train           joy   \n4  0x20b21d          train  anticipation   \n\n                                              text_x  \\\n0  Huge Respect🖒 @JohnnyVegasReal talking about l...   \n1  Yoooo we hit all our monthly goals with the ne...   \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n3  Come join @ambushman27 on #PUBG while he striv...   \n4  @fanshixieen2014 Blessings!My #strength little...   \n\n                                        text_stemmed  \\\n0  huge respect🖒 @johnnyvegasr talk about lose hi...   \n1  yoooo we hit all our monthli goal with the new...   \n2  @kidsnt @picu_bch @uhbcomm @bwchboss well done...   \n3  come join @ambushman27 on #pubg while he striv...   \n4  @fanshixieen2014 blessings!mi #strength little...   \n\n                                              text_y  year  month  day  \\\n0  Huge Respect🖒 @JohnnyVegasReal talking about l...  2015      1   17   \n1  Yoooo we hit all our monthly goals with the ne...  2016      7    2   \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  2016      8   15   \n3  Come join @ambushman27 on #PUBG while he striv...  2017      2   11   \n4  @fanshixieen2014 Blessings!My #strength little...  2016     11   23   \n\n   weekday  hour  tweet_length  hashtags_count  retweets_count  replies_count  \n0        5     3           140               1               0              1  \n1        5     9            87               2               0              0  \n2        0    18            81               0               0              4  \n3        5     8           132               6               0              1  \n4        2     5           139               4               0              1  \n   tweet_id identification                                             text_x  \\\n0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n1  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n2  0x2466f6           test  Looking for a new car, and it says 1 lady owne...   \n3  0x23f9e9           test  @cineworld “only the brave” just out and fount...   \n4  0x1fb4e1           test  Felt like total dog 💩 going into open gym and ...   \n\n                                        text_stemmed  \\\n0  @habbo i'v seen two separ colour of the eleg f...   \n1  @foxnew @kellyannepol No seriou self respect i...   \n2  look for a new car, and it say 1 ladi owner. t...   \n3  @cineworld “onli the brave” just out and fount...   \n4  felt like total dog 💩 go into open gym and had...   \n\n                                              text_y  year  month  day  \\\n0  @Habbo I've seen two separate colours of the e...  2017      1   17   \n1  @FoxNews @KellyannePolls No serious self respe...  2015     10   17   \n2  Looking for a new car, and it says 1 lady owne...  2016     12   19   \n3  @cineworld “only the brave” just out and fount...  2017      4    9   \n4  Felt like total dog 💩 going into open gym and ...  2016      1   15   \n\n   weekday  hour  tweet_length  hashtags_count  retweets_count  replies_count  \n0        1    14            81               0               0              1  \n1        5     6            99               0               0              2  \n2        0     3           116               1               0              0  \n3        6    19           105               1               0              1  \n4        4    11           137               0               0              0  \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#2-3特徵選擇(中繼資料特徵)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# 選擇特徵和目標變數\nX = final_data.drop(['tweet_id', 'identification', 'emotion', 'text_x', 'text_stemmed', 'text_y'], axis=1)\ny = final_data['emotion']\n\n# 初始化梯度提升樹模型\ngb = GradientBoostingClassifier(n_estimators=10, random_state=42)\n\n# 訓練模型\ngb.fit(X, y)\n\n# 使用模型評估特徵重要性\nimportances = gb.feature_importances_\n\n# 將特徵重要性與特徵名稱結合起來\nfeature_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n\n# 選擇平均以上的特徵\nthreshold = importances.mean()\nselected_features = feature_importances[feature_importances > threshold].index.tolist()\n\n# 創建一個新的 DataFrame，包含選中的特徵和其他非特徵列\nselected_data = final_data[['tweet_id', 'identification', 'emotion'] + selected_features]\n\n# 列印最終選擇的特徵和目標變數的前幾行\nprint(selected_data.head())\n\n# 創建一個新的 DataFrame，包含選中的特徵和其他非特徵列\nselected_test_data = final_test_data[['tweet_id', 'identification'] + selected_features]\n\n# 列印最終選擇的特徵和目標變數的前幾行\nprint(selected_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:59:31.427347Z","iopub.execute_input":"2024-12-03T15:59:31.427825Z","iopub.status.idle":"2024-12-03T16:02:53.820070Z","shell.execute_reply.started":"2024-12-03T15:59:31.427782Z","shell.execute_reply":"2024-12-03T16:02:53.818989Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  replies_count  tweet_length  \\\n0  0x29e452          train           joy              1           140   \n1  0x2b3819          train           joy              0            87   \n2  0x2a2acc          train         trust              4            81   \n3  0x2a8830          train           joy              1           132   \n4  0x20b21d          train  anticipation              1           139   \n\n   hashtags_count  \n0               1  \n1               2  \n2               0  \n3               6  \n4               4  \n   tweet_id identification  replies_count  tweet_length  hashtags_count\n0  0x28cc61           test              1            81               0\n1  0x2db41f           test              2            99               0\n2  0x2466f6           test              0           116               1\n3  0x23f9e9           test              1           105               1\n4  0x1fb4e1           test              0           137               0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#2-4特徵合併\n# 從第二個程式中獲取選擇的特徵清單\nselected_features = feature_importances[feature_importances > threshold].index.tolist()\n\n# 將text_stemmed添加到selected_features列表中\nselected_features.append('text_stemmed')\n\n# 創建一個新的DataFrame，包含選中的特徵和其他非特徵列\nselected_data = final_data[['tweet_id', 'identification', 'emotion'] + selected_features]\n\n# 列印最終選擇的特徵和目標變數的前幾行\nprint(selected_data.head())\n\n# 創建一個新的DataFrame，包含選中的特徵和其他非特徵列\nselected_test_data = final_test_data[['tweet_id', 'identification'] + selected_features]\n\n# 列印最終選擇的特徵和目標變數的前幾行\nprint(selected_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:03:52.421875Z","iopub.execute_input":"2024-12-03T16:03:52.422288Z","iopub.status.idle":"2024-12-03T16:03:52.571282Z","shell.execute_reply.started":"2024-12-03T16:03:52.422250Z","shell.execute_reply":"2024-12-03T16:03:52.570128Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  replies_count  tweet_length  \\\n0  0x29e452          train           joy              1           140   \n1  0x2b3819          train           joy              0            87   \n2  0x2a2acc          train         trust              4            81   \n3  0x2a8830          train           joy              1           132   \n4  0x20b21d          train  anticipation              1           139   \n\n   hashtags_count                                       text_stemmed  \n0               1  huge respect🖒 @johnnyvegasr talk about lose hi...  \n1               2  yoooo we hit all our monthli goal with the new...  \n2               0  @kidsnt @picu_bch @uhbcomm @bwchboss well done...  \n3               6  come join @ambushman27 on #pubg while he striv...  \n4               4  @fanshixieen2014 blessings!mi #strength little...  \n   tweet_id identification  replies_count  tweet_length  hashtags_count  \\\n0  0x28cc61           test              1            81               0   \n1  0x2db41f           test              2            99               0   \n2  0x2466f6           test              0           116               1   \n3  0x23f9e9           test              1           105               1   \n4  0x1fb4e1           test              0           137               0   \n\n                                        text_stemmed  \n0  @habbo i'v seen two separ colour of the eleg f...  \n1  @foxnew @kellyannepol No seriou self respect i...  \n2  look for a new car, and it say 1 ladi owner. t...  \n3  @cineworld “onli the brave” just out and fount...  \n4  felt like total dog 💩 go into open gym and had...  \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#3-1模型訓練(樸素貝葉斯模型)\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom joblib import dump\nfrom scipy.sparse import hstack\n\n# 創建樸素貝葉斯模型\nnb_classifier = MultinomialNB()\n\n# 準備數據\nX = selected_data.drop(['tweet_id', 'identification', 'emotion'], axis=1)\ny = selected_data['emotion']\n\n# 準備數據\nX_sample = selected_test_data.drop(['tweet_id', 'identification'], axis=1)\n\n# 文本特徵處理，優化詞彙量\nvectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2)) # 假設我們只保留最重要的20000個詞彙\nX_text = vectorizer.fit_transform(X['text_stemmed'])\nX_sample_test = vectorizer.fit_transform(X_sample['text_stemmed'])\n\n# 將其他數值特徵與文本特徵合併\nX_numeric = X.drop('text_stemmed', axis=1).values\nX_combined = hstack((X_numeric, X_text))  # 使用疏鬆陣列合併\nX_sample_numeric = X_sample.drop('text_stemmed', axis=1).values\nX_sample_combined = hstack((X_sample_numeric, X_sample_test))  # 使用疏鬆陣列合併\n\n# 交叉驗證\nscores = cross_val_score(nb_classifier, X_combined, y, cv=3)  # 使用3折交叉驗證\nprint(f\"Cross-validation scores: {scores}\")\nprint(f\"Average score: {scores.mean()}\")\n\n# 訓練模型\nnb_classifier.fit(X_combined, y)\n\n# 保存模型\ndump(nb_classifier, 'naive_bayes_model.joblib')","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:03:56.220092Z","iopub.execute_input":"2024-12-03T16:03:56.220518Z","iopub.status.idle":"2024-12-03T16:06:25.272780Z","shell.execute_reply.started":"2024-12-03T16:03:56.220482Z","shell.execute_reply":"2024-12-03T16:06:25.271511Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cross-validation scores: [0.52143705 0.52368566 0.52271598]\nAverage score: 0.522612899684235\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['naive_bayes_model.joblib']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#3-2模型優化及評估（網格搜索）\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom joblib import load\nfrom sklearn.metrics import make_scorer, f1_score\n\n# 載入已經訓練好的模型\nnb_classifier = load('naive_bayes_model.joblib')\n\n# 定義超參數網格\nparam_grid = {\n    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],\n    'fit_prior': [True, False]\n}\n\n\n# 創建F1分數評分器\nf1_scorer = make_scorer(f1_score, average='micro')  \n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5)\ngrid_search = GridSearchCV(estimator=nb_classifier, param_grid=param_grid, \n                           cv=skf, scoring={'accuracy': 'accuracy', 'f1': f1_scorer}, \n                           refit='f1', verbose=2, n_jobs=-1)\n\n# 執行網格搜索\ngrid_search.fit(X_combined, y)\n\n# 輸出最佳參數和得分\nprint(\"Best parameters found: \", grid_search.best_params_)\nprint(\"Best cross-validation scores: \")\nprint(\"Accuracy: \", grid_search.cv_results_['mean_test_accuracy'][grid_search.best_index_])\nprint(\"F1 score: \", grid_search.cv_results_['mean_test_f1'][grid_search.best_index_])\n\n# 使用最佳參數訓練模型\nbest_nb_classifier = grid_search.best_estimator_\n\n# 保存模型\ndump(best_nb_classifier, 'naive_bayes_optimized_model.joblib')","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:16:34.475918Z","iopub.execute_input":"2024-12-03T16:16:34.476472Z","iopub.status.idle":"2024-12-03T16:20:55.693006Z","shell.execute_reply.started":"2024-12-03T16:16:34.476420Z","shell.execute_reply":"2024-12-03T16:20:55.691729Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 10 candidates, totalling 50 fits\nBest parameters found:  {'alpha': 0.1, 'fit_prior': True}\nBest cross-validation scores: \nAccuracy:  0.5245379281421201\nF1 score:  0.5245379281421201\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['naive_bayes_optimized_model.joblib']"},"metadata":{}},{"name":"stdout","text":"[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.9s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.5s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.9s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.7s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.5s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  20.5s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.6s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.0s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.2s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  19.5s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.0s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.4s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.5s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.6s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  20.0s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  11.9s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.8s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.5s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.6s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  20.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.7s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.5s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  20.1s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.6s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  20.4s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.5s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  19.4s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  10.8s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  20.0s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.0s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.5s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.2s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=   9.8s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.5s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.8s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.1s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.0s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.3s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.1s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.2s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.3s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  19.9s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  10.0s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#4.生成預測\nimport joblib\nimport pandas as pd\n\n# 載入訓練好的模型\noptimized_nb_classifier = joblib.load('naive_bayes_optimized_model.joblib')\n\n# 使用模型進行預測\npredicted_emotions = optimized_nb_classifier.predict(X_sample_combined)\n\n# 創建提交文件的DataFrame，將'tweet_id'改為'id'\nsubmission_data = pd.DataFrame({\n    'id': selected_test_data['tweet_id'],  # 修改列名\n    'emotion': predicted_emotions\n})\n\n# 保存為CSV檔\nsubmission_data.to_csv('sampleSubmission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:29:08.945816Z","iopub.execute_input":"2024-12-03T16:29:08.946333Z","iopub.status.idle":"2024-12-03T16:29:10.057269Z","shell.execute_reply.started":"2024-12-03T16:29:08.946292Z","shell.execute_reply":"2024-12-03T16:29:10.056052Z"},"trusted":true},"outputs":[],"execution_count":17}]}