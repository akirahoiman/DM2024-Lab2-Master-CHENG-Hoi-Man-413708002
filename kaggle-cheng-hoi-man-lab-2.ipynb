{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87232,"databundleVersionId":9912598,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#1.æ•¸æ“šé è™•ç†\n#1-1æ•¸æ“šæ¸…æ´—-ç¼ºå¤±å€¼è™•ç†\n\nimport pandas as pd\nimport json\n\n# å®šç¾©æª”è·¯å¾‘\ndata_identification_path = r'/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv'\nemotion_path = r'/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv'\n\n# è®€å–æ•¸æ“š\ndata_identification = pd.read_csv(data_identification_path)\nemotion = pd.read_csv(emotion_path)\n\n# 1-1 è³‡æ–™æ¸…æ´— - ç¼ºå¤±å€¼è™•ç†\n# æª¢æŸ¥data_identification.csvä¸­çš„ç¼ºå¤±å€¼\nmissing_data_identification = data_identification.isnull().sum()\nprint(\"ç¼ºå¤±å€¼çµ±è¨ˆ - data_identification.csv:\")\nprint(missing_data_identification)\n\n# æª¢æŸ¥emotion.csvä¸­çš„ç¼ºå¤±å€¼\nmissing_emotion = emotion.isnull().sum()\nprint(\"ç¼ºå¤±å€¼çµ±è¨ˆ - emotion.csv:\")\nprint(missing_emotion)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-03T15:38:07.266088Z","iopub.execute_input":"2024-12-03T15:38:07.267715Z","iopub.status.idle":"2024-12-03T15:38:10.628763Z","shell.execute_reply.started":"2024-12-03T15:38:07.267608Z","shell.execute_reply":"2024-12-03T15:38:10.627587Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ç¼ºå¤±å€¼çµ±è¨ˆ - data_identification.csv:\ntweet_id          0\nidentification    0\ndtype: int64\nç¼ºå¤±å€¼çµ±è¨ˆ - emotion.csv:\ntweet_id    0\nemotion     0\ndtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# 1-2 è³‡æ–™åŠƒåˆ† - è¨“ç·´é›†å’Œæ¸¬è©¦é›†\n# æ ¹æ“šidentificationåˆ—å°‡è³‡æ–™åˆ†ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†\ntrain_data = data_identification[data_identification['identification'] == 'train'].copy()\ntest_data = data_identification[data_identification['identification'] == 'test'].copy()\n\n# æª¢æŸ¥è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­çš„tweet_idæ˜¯å¦æœ‰é‡ç–Š\né‡ç–Štweet_id = train_data['tweet_id'].isin(test_data['tweet_id']).sum()\nprint(\"è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­é‡ç–Šçš„tweet_idæ•¸é‡:\", é‡ç–Štweet_id)\n\n# å¦‚æœæœ‰é‡ç–Šï¼Œåˆ—å°å‡ºé‡ç–Šçš„tweet_id\nif é‡ç–Štweet_id > 0:\n    overlapping_ids = train_data[train_data['tweet_id'].isin(test_data['tweet_id'])]['tweet_id'].unique()\n    print(\"é‡ç–Šçš„tweet_idåˆ—è¡¨:\", overlapping_ids)\nelse:\n    print(\"è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­çš„tweet_idæ²’æœ‰é‡ç–Šã€‚\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:38:14.601253Z","iopub.execute_input":"2024-12-03T15:38:14.601744Z","iopub.status.idle":"2024-12-03T15:38:15.629785Z","shell.execute_reply.started":"2024-12-03T15:38:14.601701Z","shell.execute_reply":"2024-12-03T15:38:15.628236Z"},"trusted":true},"outputs":[{"name":"stdout","text":"è¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­é‡ç–Šçš„tweet_idæ•¸é‡: 0\nè¨“ç·´é›†å’Œæ¸¬è©¦é›†ä¸­çš„tweet_idæ²’æœ‰é‡ç–Šã€‚\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 1-3 åˆä½µè³‡æ–™é›†\n# ä½¿ç”¨tweet_idå°‡train_dataå’Œemotion.csvåˆä½µï¼Œç¢ºä¿æ¯å€‹æ¨æ–‡éƒ½æœ‰å°æ‡‰çš„æƒ…ç·’æ¨™ç±¤\nmerged_train_data = pd.merge(train_data, emotion, on='tweet_id', how='left')\n\n# æª¢æŸ¥åˆä½µå¾Œçš„ç¼ºå¤±å€¼\nmissing_values = merged_train_data.isnull().sum()\nprint(\"åˆä½µå¾Œçš„ç¼ºå¤±å€¼çµ±è¨ˆ:\")\nprint(missing_values)\n\n# æŸ¥çœ‹åˆä½µå¾Œçš„è³‡æ–™é›†å‰å¹¾è¡Œ\nprint(merged_train_data.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:38:18.476514Z","iopub.execute_input":"2024-12-03T15:38:18.476976Z","iopub.status.idle":"2024-12-03T15:38:20.667540Z","shell.execute_reply.started":"2024-12-03T15:38:18.476935Z","shell.execute_reply":"2024-12-03T15:38:20.666299Z"},"trusted":true},"outputs":[{"name":"stdout","text":"åˆä½µå¾Œçš„ç¼ºå¤±å€¼çµ±è¨ˆ:\ntweet_id          0\nidentification    0\nemotion           0\ndtype: int64\n   tweet_id identification       emotion\n0  0x29e452          train           joy\n1  0x2b3819          train           joy\n2  0x2a2acc          train         trust\n3  0x2a8830          train           joy\n4  0x20b21d          train  anticipation\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#1-4æ–‡æœ¬è³‡æ–™è®€å–\nimport os\n\n# å®šç¾©JSONæª”è·¯å¾‘\ntweets_dm_path = r'/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json'\n\n# åˆå§‹åŒ–ä¸€å€‹ç©ºå­—å…¸ä¾†å­˜å„²æ¨æ–‡è³‡æ–™\ntweets_data = {}\n\n# è®€å–JSONæ–‡ä»¶\nwith open(tweets_dm_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        try:\n            # å°‡æ¯è¡Œè§£æç‚ºJSONå°è±¡\n            tweet = json.loads(line)\n            # æå–tweet_idå’Œæ¨æ–‡æ–‡æœ¬\n            tweet_id = tweet['_source']['tweet']['tweet_id']\n            text = tweet['_source']['tweet']['text']\n            # å°‡æ¨æ–‡æ–‡æœ¬å­˜å„²åœ¨å­—å…¸ä¸­\n            tweets_data[tweet_id] = text\n        except json.JSONDecodeError as e:\n            # å¦‚æœè§£æå‡ºéŒ¯ï¼Œåˆ—å°éŒ¯èª¤è³‡è¨Šå’Œå°æ‡‰çš„è¡Œ\n            print(f\"Error parsing line: {line}\")\n            print(e)\n\n# å°‡æ¨æ–‡æ–‡æœ¬è³‡æ–™è½‰æ›ç‚ºDataFrame\ntweets_df = pd.DataFrame(list(tweets_data.items()), columns=['tweet_id', 'text'])\n\n# å°‡æ¨æ–‡æ–‡æœ¬è³‡æ–™èˆ‡åˆä½µå¾Œçš„è³‡æ–™é›†é—œè¯ï¼Œä½¿ç”¨tweet_idä½œç‚ºé—œè¯éµ\nfinal_data = pd.merge(merged_train_data, tweets_df, on='tweet_id', how='left')\n\n# æª¢æŸ¥æœ€çµ‚é—œè¯å¾Œçš„è³‡æ–™é›†å‰å¹¾è¡Œ\nprint(final_data.head())\n\n# å°‡æ¨æ–‡æ–‡æœ¬è³‡æ–™èˆ‡åˆä½µå¾Œçš„è³‡æ–™é›†é—œè¯ï¼Œä½¿ç”¨tweet_idä½œç‚ºé—œè¯éµ\nfinal_test_data = pd.merge(test_data, tweets_df, on='tweet_id', how='left')\n\n# æª¢æŸ¥æœ€çµ‚é—œè¯å¾Œçš„è³‡æ–™é›†å‰å¹¾è¡Œ\nprint(final_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:38:31.878694Z","iopub.execute_input":"2024-12-03T15:38:31.879189Z","iopub.status.idle":"2024-12-03T15:38:51.545582Z","shell.execute_reply.started":"2024-12-03T15:38:31.879148Z","shell.execute_reply":"2024-12-03T15:38:51.544360Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  \\\n0  0x29e452          train           joy   \n1  0x2b3819          train           joy   \n2  0x2a2acc          train         trust   \n3  0x2a8830          train           joy   \n4  0x20b21d          train  anticipation   \n\n                                                text  \n0  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...  \n1  Yoooo we hit all our monthly goals with the ne...  \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  \n3  Come join @ambushman27 on #PUBG while he striv...  \n4  @fanshixieen2014 Blessings!My #strength little...  \n   tweet_id identification                                               text\n0  0x28cc61           test  @Habbo I've seen two separate colours of the e...\n1  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...\n2  0x2466f6           test  Looking for a new car, and it says 1 lady owne...\n3  0x23f9e9           test  @cineworld â€œonly the braveâ€ just out and fount...\n4  0x1fb4e1           test  Felt like total dog ğŸ’© going into open gym and ...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#2.ç‰¹å¾µå·¥ç¨‹\n#2-1æ–‡æœ¬ç‰¹å¾µ(è©å¹¹æå–)\nimport nltk\nfrom nltk.stem import PorterStemmer\n\n# åˆå§‹åŒ–è©å¹¹æå–å™¨\nstemmer = PorterStemmer()\n\n# å®šç¾©è©å¹¹æå–å‡½æ•¸\ndef stemming(text):\n    words = text.split()\n    stemmed_words = [stemmer.stem(word) for word in words]\n    return ' '.join(stemmed_words)\n\n# æ‡‰ç”¨æ–‡æœ¬ç‰¹å¾µæå–å‡½æ•¸åˆ°æ¨æ–‡æ–‡æœ¬åˆ—\nfinal_data['text_stemmed'] = final_data['text'].apply(stemming)\n\n# æª¢æŸ¥æœ€çµ‚é—œè¯å¾Œçš„è³‡æ–™é›†å‰å¹¾è¡Œ\nprint(final_data.head())\n\n# æ‡‰ç”¨æ–‡æœ¬ç‰¹å¾µæå–å‡½æ•¸åˆ°æ¨æ–‡æ–‡æœ¬åˆ—\nfinal_test_data['text_stemmed'] = final_test_data['text'].apply(stemming)\n\n# æª¢æŸ¥æœ€çµ‚é—œè¯å¾Œçš„è³‡æ–™é›†å‰å¹¾è¡Œ\nprint(final_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:39:10.201146Z","iopub.execute_input":"2024-12-03T15:39:10.201575Z","iopub.status.idle":"2024-12-03T15:47:13.884625Z","shell.execute_reply.started":"2024-12-03T15:39:10.201521Z","shell.execute_reply":"2024-12-03T15:47:13.883619Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  \\\n0  0x29e452          train           joy   \n1  0x2b3819          train           joy   \n2  0x2a2acc          train         trust   \n3  0x2a8830          train           joy   \n4  0x20b21d          train  anticipation   \n\n                                                text  \\\n0  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...   \n1  Yoooo we hit all our monthly goals with the ne...   \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n3  Come join @ambushman27 on #PUBG while he striv...   \n4  @fanshixieen2014 Blessings!My #strength little...   \n\n                                        text_stemmed  \n0  huge respectğŸ–’ @johnnyvegasr talk about lose hi...  \n1  yoooo we hit all our monthli goal with the new...  \n2  @kidsnt @picu_bch @uhbcomm @bwchboss well done...  \n3  come join @ambushman27 on #pubg while he striv...  \n4  @fanshixieen2014 blessings!mi #strength little...  \n   tweet_id identification                                               text  \\\n0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n1  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n2  0x2466f6           test  Looking for a new car, and it says 1 lady owne...   \n3  0x23f9e9           test  @cineworld â€œonly the braveâ€ just out and fount...   \n4  0x1fb4e1           test  Felt like total dog ğŸ’© going into open gym and ...   \n\n                                        text_stemmed  \n0  @habbo i'v seen two separ colour of the eleg f...  \n1  @foxnew @kellyannepol No seriou self respect i...  \n2  look for a new car, and it say 1 ladi owner. t...  \n3  @cineworld â€œonli the braveâ€ just out and fount...  \n4  felt like total dog ğŸ’© go into open gym and had...  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#2-2ä¸­ç¹¼è³‡æ–™ç‰¹å¾µ(æ™‚é–“ç‰¹å¾µã€ä½¿ç”¨è€…è¡Œç‚ºç‰¹å¾µã€tweeté•·åº¦)\nimport datetime\n\n# å®šç¾©æå–æ™‚é–“ç‰¹å¾µçš„å‡½æ•¸\ndef extract_time_features(date_str):\n    date = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n    return {\n        'year': date.year,\n        'month': date.month,\n        'day': date.day,\n        'weekday': date.weekday(),  # 0 is Monday, 6 is Sunday\n        'hour': date.hour\n    }\n\n# å®šç¾©æå–ä½¿ç”¨è€…è¡Œç‚ºç‰¹å¾µçš„å‡½æ•¸\ndef extract_user_behavior_features(text):\n    hashtags = text.count('#')\n    retweet = text.count('RT ')\n    reply = text.count('@')\n    return {\n        'hashtags_count': hashtags,\n        'retweets_count': retweet,\n        'replies_count': reply\n    }\n\n# å®šç¾©æå–tweeté•·åº¦ç‰¹å¾µçš„å‡½æ•¸\ndef tweet_length(text):\n    return len(text)\n\n# è®€å–tweets_DM.jsonæª”ä¸¦æå–ç‰¹å¾µ\ntweets_features = []\n\nwith open(tweets_dm_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        try:\n            tweet = json.loads(line)\n            tweet_id = tweet['_source']['tweet']['tweet_id']\n            crawl_date = tweet['_crawldate']\n            \n            # æå–æ™‚é–“ç‰¹å¾µ\n            time_features = extract_time_features(crawl_date)\n            \n            # å°‡ç‰¹å¾µçµ„åˆåœ¨ä¸€èµ·\n            tweets_features.append({\n                'tweet_id': tweet_id,\n                **time_features\n            })\n        except json.JSONDecodeError as e:\n            print(f\"Error parsing line: {line}\")\n            print(e)\n\n# å°‡ç‰¹å¾µè½‰æ›ç‚ºDataFrame\ntweets_time_features_df = pd.DataFrame(tweets_features)\n\n# åˆä½µæ™‚é–“ç‰¹å¾µåˆ°tweets_df\ntweets_df = pd.merge(tweets_df, tweets_time_features_df, on='tweet_id', how='left')\n\n# æå–ä½¿ç”¨è€…è¡Œç‚ºç‰¹å¾µå’Œtweeté•·åº¦ç‰¹å¾µ\ntweets_df['user_behavior_features'] = tweets_df['text'].apply(extract_user_behavior_features)\ntweets_df['tweet_length'] = tweets_df['text'].apply(tweet_length)\n\n# å±•é–‹ä½¿ç”¨è€…è¡Œç‚ºç‰¹å¾µ\nuser_behavior_columns = ['hashtags_count', 'retweets_count', 'replies_count']\ntweets_df[user_behavior_columns] = pd.DataFrame(tweets_df['user_behavior_features'].tolist(), index=tweets_df.index)\n\n# åˆªé™¤è‡¨æ™‚åˆ—\ntweets_df.drop(['user_behavior_features'], axis=1, inplace=True)\n\n# åˆä½µæœ€çµ‚è³‡æ–™é›†\nfinal_data = pd.merge(final_data, tweets_df, on='tweet_id', how='left')\n\n# æª¢æŸ¥æœ€çµ‚è³‡æ–™é›†çš„å‰å¹¾è¡Œ\nprint(final_data.head())\n\n# åˆä½µæœ€çµ‚è³‡æ–™é›†\nfinal_test_data = pd.merge(final_test_data, tweets_df, on='tweet_id', how='left')\n\n# æª¢æŸ¥æœ€çµ‚è³‡æ–™é›†çš„å‰å¹¾è¡Œ\nprint(final_test_data.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:56:12.120696Z","iopub.execute_input":"2024-12-03T15:56:12.121112Z","iopub.status.idle":"2024-12-03T15:57:02.444796Z","shell.execute_reply.started":"2024-12-03T15:56:12.121078Z","shell.execute_reply":"2024-12-03T15:57:02.443674Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  \\\n0  0x29e452          train           joy   \n1  0x2b3819          train           joy   \n2  0x2a2acc          train         trust   \n3  0x2a8830          train           joy   \n4  0x20b21d          train  anticipation   \n\n                                              text_x  \\\n0  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...   \n1  Yoooo we hit all our monthly goals with the ne...   \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...   \n3  Come join @ambushman27 on #PUBG while he striv...   \n4  @fanshixieen2014 Blessings!My #strength little...   \n\n                                        text_stemmed  \\\n0  huge respectğŸ–’ @johnnyvegasr talk about lose hi...   \n1  yoooo we hit all our monthli goal with the new...   \n2  @kidsnt @picu_bch @uhbcomm @bwchboss well done...   \n3  come join @ambushman27 on #pubg while he striv...   \n4  @fanshixieen2014 blessings!mi #strength little...   \n\n                                              text_y  year  month  day  \\\n0  Huge RespectğŸ–’ @JohnnyVegasReal talking about l...  2015      1   17   \n1  Yoooo we hit all our monthly goals with the ne...  2016      7    2   \n2  @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  2016      8   15   \n3  Come join @ambushman27 on #PUBG while he striv...  2017      2   11   \n4  @fanshixieen2014 Blessings!My #strength little...  2016     11   23   \n\n   weekday  hour  tweet_length  hashtags_count  retweets_count  replies_count  \n0        5     3           140               1               0              1  \n1        5     9            87               2               0              0  \n2        0    18            81               0               0              4  \n3        5     8           132               6               0              1  \n4        2     5           139               4               0              1  \n   tweet_id identification                                             text_x  \\\n0  0x28cc61           test  @Habbo I've seen two separate colours of the e...   \n1  0x2db41f           test  @FoxNews @KellyannePolls No serious self respe...   \n2  0x2466f6           test  Looking for a new car, and it says 1 lady owne...   \n3  0x23f9e9           test  @cineworld â€œonly the braveâ€ just out and fount...   \n4  0x1fb4e1           test  Felt like total dog ğŸ’© going into open gym and ...   \n\n                                        text_stemmed  \\\n0  @habbo i'v seen two separ colour of the eleg f...   \n1  @foxnew @kellyannepol No seriou self respect i...   \n2  look for a new car, and it say 1 ladi owner. t...   \n3  @cineworld â€œonli the braveâ€ just out and fount...   \n4  felt like total dog ğŸ’© go into open gym and had...   \n\n                                              text_y  year  month  day  \\\n0  @Habbo I've seen two separate colours of the e...  2017      1   17   \n1  @FoxNews @KellyannePolls No serious self respe...  2015     10   17   \n2  Looking for a new car, and it says 1 lady owne...  2016     12   19   \n3  @cineworld â€œonly the braveâ€ just out and fount...  2017      4    9   \n4  Felt like total dog ğŸ’© going into open gym and ...  2016      1   15   \n\n   weekday  hour  tweet_length  hashtags_count  retweets_count  replies_count  \n0        1    14            81               0               0              1  \n1        5     6            99               0               0              2  \n2        0     3           116               1               0              0  \n3        6    19           105               1               0              1  \n4        4    11           137               0               0              0  \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#2-3ç‰¹å¾µé¸æ“‡(ä¸­ç¹¼è³‡æ–™ç‰¹å¾µ)\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# é¸æ“‡ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸\nX = final_data.drop(['tweet_id', 'identification', 'emotion', 'text_x', 'text_stemmed', 'text_y'], axis=1)\ny = final_data['emotion']\n\n# åˆå§‹åŒ–æ¢¯åº¦æå‡æ¨¹æ¨¡å‹\ngb = GradientBoostingClassifier(n_estimators=10, random_state=42)\n\n# è¨“ç·´æ¨¡å‹\ngb.fit(X, y)\n\n# ä½¿ç”¨æ¨¡å‹è©•ä¼°ç‰¹å¾µé‡è¦æ€§\nimportances = gb.feature_importances_\n\n# å°‡ç‰¹å¾µé‡è¦æ€§èˆ‡ç‰¹å¾µåç¨±çµåˆèµ·ä¾†\nfeature_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n\n# é¸æ“‡å¹³å‡ä»¥ä¸Šçš„ç‰¹å¾µ\nthreshold = importances.mean()\nselected_features = feature_importances[feature_importances > threshold].index.tolist()\n\n# å‰µå»ºä¸€å€‹æ–°çš„ DataFrameï¼ŒåŒ…å«é¸ä¸­çš„ç‰¹å¾µå’Œå…¶ä»–éç‰¹å¾µåˆ—\nselected_data = final_data[['tweet_id', 'identification', 'emotion'] + selected_features]\n\n# åˆ—å°æœ€çµ‚é¸æ“‡çš„ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸çš„å‰å¹¾è¡Œ\nprint(selected_data.head())\n\n# å‰µå»ºä¸€å€‹æ–°çš„ DataFrameï¼ŒåŒ…å«é¸ä¸­çš„ç‰¹å¾µå’Œå…¶ä»–éç‰¹å¾µåˆ—\nselected_test_data = final_test_data[['tweet_id', 'identification'] + selected_features]\n\n# åˆ—å°æœ€çµ‚é¸æ“‡çš„ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸çš„å‰å¹¾è¡Œ\nprint(selected_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T15:59:31.427347Z","iopub.execute_input":"2024-12-03T15:59:31.427825Z","iopub.status.idle":"2024-12-03T16:02:53.820070Z","shell.execute_reply.started":"2024-12-03T15:59:31.427782Z","shell.execute_reply":"2024-12-03T16:02:53.818989Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  replies_count  tweet_length  \\\n0  0x29e452          train           joy              1           140   \n1  0x2b3819          train           joy              0            87   \n2  0x2a2acc          train         trust              4            81   \n3  0x2a8830          train           joy              1           132   \n4  0x20b21d          train  anticipation              1           139   \n\n   hashtags_count  \n0               1  \n1               2  \n2               0  \n3               6  \n4               4  \n   tweet_id identification  replies_count  tweet_length  hashtags_count\n0  0x28cc61           test              1            81               0\n1  0x2db41f           test              2            99               0\n2  0x2466f6           test              0           116               1\n3  0x23f9e9           test              1           105               1\n4  0x1fb4e1           test              0           137               0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#2-4ç‰¹å¾µåˆä½µ\n# å¾ç¬¬äºŒå€‹ç¨‹å¼ä¸­ç²å–é¸æ“‡çš„ç‰¹å¾µæ¸…å–®\nselected_features = feature_importances[feature_importances > threshold].index.tolist()\n\n# å°‡text_stemmedæ·»åŠ åˆ°selected_featuresåˆ—è¡¨ä¸­\nselected_features.append('text_stemmed')\n\n# å‰µå»ºä¸€å€‹æ–°çš„DataFrameï¼ŒåŒ…å«é¸ä¸­çš„ç‰¹å¾µå’Œå…¶ä»–éç‰¹å¾µåˆ—\nselected_data = final_data[['tweet_id', 'identification', 'emotion'] + selected_features]\n\n# åˆ—å°æœ€çµ‚é¸æ“‡çš„ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸çš„å‰å¹¾è¡Œ\nprint(selected_data.head())\n\n# å‰µå»ºä¸€å€‹æ–°çš„DataFrameï¼ŒåŒ…å«é¸ä¸­çš„ç‰¹å¾µå’Œå…¶ä»–éç‰¹å¾µåˆ—\nselected_test_data = final_test_data[['tweet_id', 'identification'] + selected_features]\n\n# åˆ—å°æœ€çµ‚é¸æ“‡çš„ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸çš„å‰å¹¾è¡Œ\nprint(selected_test_data.head())","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:03:52.421875Z","iopub.execute_input":"2024-12-03T16:03:52.422288Z","iopub.status.idle":"2024-12-03T16:03:52.571282Z","shell.execute_reply.started":"2024-12-03T16:03:52.422250Z","shell.execute_reply":"2024-12-03T16:03:52.570128Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   tweet_id identification       emotion  replies_count  tweet_length  \\\n0  0x29e452          train           joy              1           140   \n1  0x2b3819          train           joy              0            87   \n2  0x2a2acc          train         trust              4            81   \n3  0x2a8830          train           joy              1           132   \n4  0x20b21d          train  anticipation              1           139   \n\n   hashtags_count                                       text_stemmed  \n0               1  huge respectğŸ–’ @johnnyvegasr talk about lose hi...  \n1               2  yoooo we hit all our monthli goal with the new...  \n2               0  @kidsnt @picu_bch @uhbcomm @bwchboss well done...  \n3               6  come join @ambushman27 on #pubg while he striv...  \n4               4  @fanshixieen2014 blessings!mi #strength little...  \n   tweet_id identification  replies_count  tweet_length  hashtags_count  \\\n0  0x28cc61           test              1            81               0   \n1  0x2db41f           test              2            99               0   \n2  0x2466f6           test              0           116               1   \n3  0x23f9e9           test              1           105               1   \n4  0x1fb4e1           test              0           137               0   \n\n                                        text_stemmed  \n0  @habbo i'v seen two separ colour of the eleg f...  \n1  @foxnew @kellyannepol No seriou self respect i...  \n2  look for a new car, and it say 1 ladi owner. t...  \n3  @cineworld â€œonli the braveâ€ just out and fount...  \n4  felt like total dog ğŸ’© go into open gym and had...  \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#3-1æ¨¡å‹è¨“ç·´(æ¨¸ç´ è²è‘‰æ–¯æ¨¡å‹)\nimport numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom joblib import dump\nfrom scipy.sparse import hstack\n\n# å‰µå»ºæ¨¸ç´ è²è‘‰æ–¯æ¨¡å‹\nnb_classifier = MultinomialNB()\n\n# æº–å‚™æ•¸æ“š\nX = selected_data.drop(['tweet_id', 'identification', 'emotion'], axis=1)\ny = selected_data['emotion']\n\n# æº–å‚™æ•¸æ“š\nX_sample = selected_test_data.drop(['tweet_id', 'identification'], axis=1)\n\n# æ–‡æœ¬ç‰¹å¾µè™•ç†ï¼Œå„ªåŒ–è©å½™é‡\nvectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2)) # å‡è¨­æˆ‘å€‘åªä¿ç•™æœ€é‡è¦çš„20000å€‹è©å½™\nX_text = vectorizer.fit_transform(X['text_stemmed'])\nX_sample_test = vectorizer.fit_transform(X_sample['text_stemmed'])\n\n# å°‡å…¶ä»–æ•¸å€¼ç‰¹å¾µèˆ‡æ–‡æœ¬ç‰¹å¾µåˆä½µ\nX_numeric = X.drop('text_stemmed', axis=1).values\nX_combined = hstack((X_numeric, X_text))  # ä½¿ç”¨ç–é¬†é™£åˆ—åˆä½µ\nX_sample_numeric = X_sample.drop('text_stemmed', axis=1).values\nX_sample_combined = hstack((X_sample_numeric, X_sample_test))  # ä½¿ç”¨ç–é¬†é™£åˆ—åˆä½µ\n\n# äº¤å‰é©—è­‰\nscores = cross_val_score(nb_classifier, X_combined, y, cv=3)  # ä½¿ç”¨3æŠ˜äº¤å‰é©—è­‰\nprint(f\"Cross-validation scores: {scores}\")\nprint(f\"Average score: {scores.mean()}\")\n\n# è¨“ç·´æ¨¡å‹\nnb_classifier.fit(X_combined, y)\n\n# ä¿å­˜æ¨¡å‹\ndump(nb_classifier, 'naive_bayes_model.joblib')","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:03:56.220092Z","iopub.execute_input":"2024-12-03T16:03:56.220518Z","iopub.status.idle":"2024-12-03T16:06:25.272780Z","shell.execute_reply.started":"2024-12-03T16:03:56.220482Z","shell.execute_reply":"2024-12-03T16:06:25.271511Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cross-validation scores: [0.52143705 0.52368566 0.52271598]\nAverage score: 0.522612899684235\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['naive_bayes_model.joblib']"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#3-2æ¨¡å‹å„ªåŒ–åŠè©•ä¼°ï¼ˆç¶²æ ¼æœç´¢ï¼‰\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom joblib import load\nfrom sklearn.metrics import make_scorer, f1_score\n\n# è¼‰å…¥å·²ç¶“è¨“ç·´å¥½çš„æ¨¡å‹\nnb_classifier = load('naive_bayes_model.joblib')\n\n# å®šç¾©è¶…åƒæ•¸ç¶²æ ¼\nparam_grid = {\n    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],\n    'fit_prior': [True, False]\n}\n\n\n# å‰µå»ºF1åˆ†æ•¸è©•åˆ†å™¨\nf1_scorer = make_scorer(f1_score, average='micro')  \n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5)\ngrid_search = GridSearchCV(estimator=nb_classifier, param_grid=param_grid, \n                           cv=skf, scoring={'accuracy': 'accuracy', 'f1': f1_scorer}, \n                           refit='f1', verbose=2, n_jobs=-1)\n\n# åŸ·è¡Œç¶²æ ¼æœç´¢\ngrid_search.fit(X_combined, y)\n\n# è¼¸å‡ºæœ€ä½³åƒæ•¸å’Œå¾—åˆ†\nprint(\"Best parameters found: \", grid_search.best_params_)\nprint(\"Best cross-validation scores: \")\nprint(\"Accuracy: \", grid_search.cv_results_['mean_test_accuracy'][grid_search.best_index_])\nprint(\"F1 score: \", grid_search.cv_results_['mean_test_f1'][grid_search.best_index_])\n\n# ä½¿ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æ¨¡å‹\nbest_nb_classifier = grid_search.best_estimator_\n\n# ä¿å­˜æ¨¡å‹\ndump(best_nb_classifier, 'naive_bayes_optimized_model.joblib')","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:16:34.475918Z","iopub.execute_input":"2024-12-03T16:16:34.476472Z","iopub.status.idle":"2024-12-03T16:20:55.693006Z","shell.execute_reply.started":"2024-12-03T16:16:34.476420Z","shell.execute_reply":"2024-12-03T16:20:55.691729Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 10 candidates, totalling 50 fits\nBest parameters found:  {'alpha': 0.1, 'fit_prior': True}\nBest cross-validation scores: \nAccuracy:  0.5245379281421201\nF1 score:  0.5245379281421201\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['naive_bayes_optimized_model.joblib']"},"metadata":{}},{"name":"stdout","text":"[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.9s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.5s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.9s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.7s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.5s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  20.5s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.6s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.0s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.2s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  19.5s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.0s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.4s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.5s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.6s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  20.0s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  11.9s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.8s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.5s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.6s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  20.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.7s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.5s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  20.1s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  19.6s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  20.4s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.5s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  19.4s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  10.8s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  20.0s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.0s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  19.5s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.2s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=   9.8s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.5s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.8s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.3s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.1s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  19.9s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  20.2s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.0s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=0.1, fit_prior=True; total time=  19.7s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.3s\n[CV] END .........................alpha=0.1, fit_prior=False; total time=  19.1s\n[CV] END ..........................alpha=0.5, fit_prior=True; total time=  20.4s\n[CV] END .........................alpha=0.5, fit_prior=False; total time=  20.2s\n[CV] END ..........................alpha=1.0, fit_prior=True; total time=  19.2s\n[CV] END .........................alpha=1.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.3s\n[CV] END ..........................alpha=2.0, fit_prior=True; total time=  19.6s\n[CV] END .........................alpha=2.0, fit_prior=False; total time=  20.1s\n[CV] END ..........................alpha=5.0, fit_prior=True; total time=  19.3s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  19.9s\n[CV] END .........................alpha=5.0, fit_prior=False; total time=  10.0s\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"#4.ç”Ÿæˆé æ¸¬\nimport joblib\nimport pandas as pd\n\n# è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹\noptimized_nb_classifier = joblib.load('naive_bayes_optimized_model.joblib')\n\n# ä½¿ç”¨æ¨¡å‹é€²è¡Œé æ¸¬\npredicted_emotions = optimized_nb_classifier.predict(X_sample_combined)\n\n# å‰µå»ºæäº¤æ–‡ä»¶çš„DataFrameï¼Œå°‡'tweet_id'æ”¹ç‚º'id'\nsubmission_data = pd.DataFrame({\n    'id': selected_test_data['tweet_id'],  # ä¿®æ”¹åˆ—å\n    'emotion': predicted_emotions\n})\n\n# ä¿å­˜ç‚ºCSVæª”\nsubmission_data.to_csv('sampleSubmission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T16:29:08.945816Z","iopub.execute_input":"2024-12-03T16:29:08.946333Z","iopub.status.idle":"2024-12-03T16:29:10.057269Z","shell.execute_reply.started":"2024-12-03T16:29:08.946292Z","shell.execute_reply":"2024-12-03T16:29:10.056052Z"},"trusted":true},"outputs":[],"execution_count":17}]}